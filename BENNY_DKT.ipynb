{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "uuid": "99dc22a8-b63d-46fa-9fe2-810797c10e8f"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# ### 加了五倍交叉验证的DKT（导入相关包）\n",
    "# 基于CCNU的数据集\n",
    "# 2566个知识点\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "# 加了五倍交叉验证的DKT\n",
    "import os\n",
    "import datetime\n",
    "import random\n",
    "import time\n",
    "import csv\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import roc_curve, auc, mean_squared_error, mean_absolute_error, accuracy_score\n",
    "# sklearn.metrics:包含了许多模型评估指标，例如决定系数R2、准确度等;\n",
    "# roc_curve:roc曲线；mean_squared_error：均方差；mean_absolute_error：平均绝对误差；accuracy_score:准确率\n",
    "import argparse\n",
    "# 用于从 sys.argv 中解析命令项选项与参数的模块\n",
    "import numpy as np\n",
    "\n",
    "# ### 定义相关函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "uuid": "506fc7de-e4b0-4b69-afef-681b55d835b3"
   },
   "outputs": [],
   "source": [
    "# In[10]:\n",
    "\n",
    "\n",
    "# 是否适用gpu，cuda()用于将变量传输到GPU上，gpu版本是torch.cuda.FloatTensor,cpu版本是torch.FloatTensor\n",
    "use_cuda = False\n",
    "# use_cuda = True\n",
    "# torch.cuda.set_device(1)\n",
    "\n",
    "# 定义cuda()函数：参数为o，如果use_cuda为真返回o.cuda(),为假返回o\n",
    "cuda = lambda o: o.cuda() if use_cuda else o\n",
    "# torch.Tensor是默认的tensor类型（torch.FlaotTensor）的简称。\n",
    "tensor = lambda o: cuda(torch.tensor(o))\n",
    "# 生成对角线全1，其余部分全0的二维数组,函数原型：torch.eye(n, m=None, out=None)，m (int) ：列数.如果为None,则默认为n。\n",
    "eye = lambda d: cuda(torch.eye(d))\n",
    "# 返回一个形状为为size,类型为torch.dtype，里面的每一个值都是0的tensor。\n",
    "zeros = lambda *args: cuda(torch.zeros(*args))\n",
    "\n",
    "# 截断反向传播的梯度流,返回一个新的Variable即tensor，从当前计算图中分离下来的，但是仍指向原变量的存放位置,\n",
    "# 不同之处只是它的requires_grad是false，也就是说这个Variable永远不需要计算其梯度，不具有grad。\n",
    "detach = lambda o: o.cpu().detach().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "uuid": "11b74bdd-5cc8-453e-9da5-bf3cc538a71a"
   },
   "outputs": [],
   "source": [
    "def set_seed(seed=0):\n",
    "    # seed()方法改变随机数生成器的种子，可以在调用其他随机模块函数之前调用此函数。random:随机数生成器，seed:种子\n",
    "    random.seed(seed)\n",
    "    # 为CPU设置种子用于生成随机数\n",
    "    torch.manual_seed(seed)\n",
    "    # 为当前GPU设置随机种子,如果使用多个GPU，应该使用torch.cuda.manual_seed_all()为所有的GPU设置种子。\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    '''\n",
    "    置为True的话，每次返回的卷积算法将是确定的，即默认算法。如果配合上设置 Torch 的随机种子为固定值的话，\n",
    "    应该可以保证每次运行网络的时候相同输入的输出是固定的。（说人话就是让每次跑出来的效果是一致的）\n",
    "    '''\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    '''\n",
    "     置为True的话会让程序在开始时花费一点额外时间，为整个网络的每个卷积层搜索最适合它的卷积实现算法，进而实现网络的加速。\n",
    "    适用场景是网络结构固定（不是动态变化的），网络的输入形状（包括 batch size，图片大小，输入的通道）是不变的，\n",
    "    其实也就是一般情况下都比较适用。反之，如果卷积层的设置一直变化，将会导致程序不停地做优化，反而会耗费更多的时间。\n",
    "    '''\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "# ### 加载数据集\n",
    "\n",
    "# In[11]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "uuid": "e8be0c0f-ebfd-44da-9cdf-e21a3df19ff1"
   },
   "outputs": [],
   "source": [
    "# 读取数据集\n",
    "# 数据集情况：题目序列的长度 题目序列 答对的情况\n",
    "class Data:\n",
    "\n",
    "    def __init__(self, file, length, q_num, is_test=False, index_split=None, is_train=False):\n",
    "        '''\n",
    "        len: 4\n",
    "        q: 53,54,53,54\n",
    "        y: 0,0,0,0\n",
    "        t1: 0,1,2,0\n",
    "        t2: 0,1,3,5\n",
    "        t3: 3,1,2,1\n",
    "        '''\n",
    "        # 读取csv文件，delimiter说明分割字段的字符串为逗号\n",
    "        rows = csv.reader(file, delimiter=',')\n",
    "        # rows为:[[题目个数], [题目序列], [答对情况]……]\n",
    "        rows = [[int(e) for e in row if e != ''] for row in rows]\n",
    "\n",
    "        q_rows, r_rows = [], []\n",
    "\n",
    "        student_num = 0\n",
    "        # zip()将对象中对应的元素打包成一个个元组，然后返回由这些元组组成的对象,注意：不是列表\n",
    "        if is_test:\n",
    "            # 双冒号：实质为lis[start:end:step]，end没写就是到列表的最后，意思就是从索引为start到end，步长为step进行切片，每个step取一次\n",
    "            # q_row, r_row：题目序号列表，答对情况列表\n",
    "            for q_row, r_row in zip(rows[1::3], rows[2::3]):\n",
    "                num = len(q_row)\n",
    "                n = num // length\n",
    "                for i in range(n + 1):\n",
    "                    q_rows.append(q_row[i * length: (i + 1) * length])\n",
    "                    r_rows.append(r_row[i * length: (i + 1) * length])\n",
    "        else:\n",
    "            if is_train:\n",
    "                for q_row, r_row in zip(rows[1::3], rows[2::3]):\n",
    "\n",
    "                    if student_num not in index_split:\n",
    "\n",
    "                        num = len(q_row)\n",
    "\n",
    "                        n = num // length\n",
    "\n",
    "                        for i in range(n + 1):\n",
    "                            q_rows.append(q_row[i * length: (i + 1) * length])\n",
    "\n",
    "                            r_rows.append(r_row[i * length: (i + 1) * length])\n",
    "                    student_num += 1\n",
    "            # 验证集\n",
    "            else:\n",
    "                for q_row, r_row in zip(rows[1::3], rows[2::3]):\n",
    "\n",
    "                    if student_num in index_split:\n",
    "\n",
    "                        num = len(q_row)\n",
    "\n",
    "                        n = num // length\n",
    "\n",
    "                        for i in range(n + 1):\n",
    "                            q_rows.append(q_row[i * length: (i + 1) * length])\n",
    "\n",
    "                            r_rows.append(r_row[i * length: (i + 1) * length])\n",
    "                    student_num += 1\n",
    "\n",
    "        q_rows = [row for row in q_rows if len(row) > 2]\n",
    "\n",
    "        r_rows = [row for row in r_rows if len(row) > 2]\n",
    "\n",
    "        # q_min = min([min(row) for row in q_rows])\n",
    "\n",
    "        # q_rows = [[q - q_min for q in row] for row in q_rows]\n",
    "\n",
    "        self.r_rows = r_rows\n",
    "\n",
    "        # self.q_num = max([max(row) for row in q_rows]) + 1\n",
    "        self.q_num = q_num\n",
    "        self.q_rows = q_rows\n",
    "\n",
    "    # 获取[[题号,答对]，[题号,答对]，……]列表\n",
    "    def __getitem__(self, index):\n",
    "        return list(\n",
    "            zip(self.q_rows[index], self.r_rows[index]))\n",
    "\n",
    "    # 批次大小\n",
    "    def __len__(self):\n",
    "        return len(self.q_rows)\n",
    "\n",
    "\n",
    "# ### 处理样本数据（将input处理成合适的维度或shape）\n",
    "\n",
    "# In[19]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "uuid": "9863660a-8c80-411c-9809-30a0740011ce"
   },
   "outputs": [],
   "source": [
    "def collate(batch, q_num):\n",
    "    # print(\"1\",batch) # 列表：[[(题目，答案)，(题目，答案)，(题目，答案)……][(题目，答案)，……]……],32个包含一定数量(题目，答案)的列表\n",
    "    lens = [len(row) for row in batch]\n",
    "    # 最大题目数量\n",
    "    max_len = max(lens)\n",
    "    batch = tensor([[[*e, 1] for e in row] + [[0, 0, 0]] * (max_len - len(row)) for row in batch])\n",
    "    Q, Y, S = batch.T  # Q:问题，Y:预测，S:padding,样本数据缺失或者说不够时填充[[0,0,0]]张量\n",
    "    Q, Y, S = Q.T, Y.T, S.T  # torch.size([32,200])\n",
    "    X = Q + q_num * Y  # 由于类别不只是0 和 1 了，所以加上Y才正确\n",
    "    return X, Y, S, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "uuid": "628b686c-850b-4b16-9445-061064cbf676"
   },
   "outputs": [],
   "source": [
    "# ### DKT模型\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "class DKT(nn.Module):\n",
    "    def __init__(self, q_num, h_num):\n",
    "        # 调用父类，解决多重继承问题\n",
    "        super(DKT, self).__init__()\n",
    "        drop_prob1, drop_prob2 = 0.2, 0.4\n",
    "        # 隐藏状态的特征数\n",
    "        self.h_num = h_num\n",
    "        # q_num：处理过后训练集的题目个数，\n",
    "        print(\"q_num=\", q_num)\n",
    "        # ???\n",
    "        self.x_onehot = eye(3 * q_num)  # 现在是一个三分类问题所以需要乘以3\n",
    "\n",
    "        # nn.RNN(input_size, hidden_size, nlayers, nonlinearity='tanh',……):batch_first为True的话，则输入和输出张量为（batch，seq，feature）\n",
    "        self.rnn = nn.RNN(q_num * 3, h_num, num_layers=2, batch_first=True)\n",
    "        # self.rnn = nn.LSTM(q_num * 2, h_num, num_layers=2, batch_first=True, dropout=drop_prob1) # lstm版rnn\n",
    "\n",
    "        # torch.nn.Sequential是一个Sequential容器，函数模块将按照构造函数中传递的顺序添加到模块中。另外，也可以传入一个模块的有序字典。\n",
    "        # 利用torch.nn.Sequential来快速搭建一个神经网络\n",
    "        self.FCs = nn.Sequential(\n",
    "            # 激活函数\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(h_num, q_num),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        size, length = X.shape  # X.shape:(32,200),X为二维张量，来自于collate函数\n",
    "        # onehot编码\n",
    "        X = self.x_onehot[X]\n",
    "        X, hidden = self.rnn(X, zeros(2, size, self.h_num))\n",
    "        P = self.FCs(X)\n",
    "        return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "uuid": "cb7cde94-300d-4751-88c5-eb361521bcb9"
   },
   "outputs": [],
   "source": [
    "# ### 训练模型+验证+测试\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "def train(model, data, optimizer, batch_size):\n",
    "    '''\n",
    "    使用PyTorch进行训练和测试时一定注意要把实例化的model指定train/eval。\n",
    "    在model(test)之前，需要加上model.eval()，框架会自动把BN(BatchNormalization)和DropOut固定住，不会取平均，而是用训练好的值。\n",
    "    否则的话，有输入数据（test的batch_size过小，），即使不训练，它也会改变权值，这是model中含有batch normalization层所带来的的性质。\n",
    "    model.train()：启用 BatchNormalization 和 Dropout\n",
    "    model.eval()：不启用 BatchNormalization 和 Dropout\n",
    "    '''\n",
    "    model.train(mode=True)\n",
    "    # 创建一个测量目标和输出之间的二进制交叉熵的标准\n",
    "    criterion = nn.CrossEntropyLoss()  # 多分类问题中\n",
    "    # criterion = nn.nll_loss()\n",
    "    q_onehot = eye(data.q_num)\n",
    "\n",
    "    for X, Y, S, Q in DataLoader(\n",
    "            dataset=data,\n",
    "            batch_size=batch_size,\n",
    "            collate_fn=lambda batch: collate(batch, data.q_num),\n",
    "            shuffle=True\n",
    "    ):\n",
    "        P = model(X)\n",
    "        Q = q_onehot[Q]\n",
    "        Q, P, Y, S = Q[:, 1:], P[:, :-1], Y[:, 1:], S[:, 1:]\n",
    "\n",
    "        P = (Q * P).sum(2)\n",
    "\n",
    "        index = S == 1\n",
    "\n",
    "        P = torch.sigmoid(P)\n",
    "        dkt_y_hat = P[index]\n",
    "        y = Y[index]\n",
    "\n",
    "        # 在这里吧输出数据当初分类网络的输入数据，主要是把输出数据进行分类\n",
    "        class_input_size = 1  # 此时上游输出已经是各个题目的掌握程度\n",
    "        class_rnn_hidden_size = 128  # 这个\n",
    "        class_rnn = nn.RNN(input_size=class_input_size, hidden_size=class_rnn_hidden_size, num_layers=2,\n",
    "                           batch_first=True)\n",
    "        class_rnn_fc = nn.Sequential(\n",
    "            # 激活函数\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(class_rnn_hidden_size, 3),\n",
    "            nn.Softmax(dim=2),\n",
    "        )\n",
    "        length = dkt_y_hat.shape[0]\n",
    "        new_shape = (1,length,1)\n",
    "        dkt_y_hat = torch.reshape(dkt_y_hat, new_shape)\n",
    "        size = dkt_y_hat.shape[0]\n",
    "        dkt_y_hat, hidden = class_rnn(dkt_y_hat, zeros(2, size, class_rnn_hidden_size))\n",
    "        dkt_y_hat = class_rnn_fc(dkt_y_hat)\n",
    "        dkt_y_hat = torch.reshape(dkt_y_hat,(length,3))\n",
    "        # 损失函数计算\n",
    "        loss = criterion(dkt_y_hat,y)\n",
    "\n",
    "        # 把模型中参数的梯度设为0\n",
    "        optimizer.zero_grad()\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        # 一步梯度下降，执行单个优化步骤（参数更新）\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "uuid": "4e097113-0dc7-479e-930e-13c1bde927ad"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, data, batch_size):\n",
    "    model.eval()\n",
    "    # criterion = nn.BCELoss()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    y_pred, y_true = [], []\n",
    "    loss = 0.0\n",
    "    q_onehot = eye(data.q_num)\n",
    "    for X, Y, S, Q in DataLoader(\n",
    "            dataset=data,\n",
    "            batch_size=batch_size,\n",
    "            collate_fn=lambda batch: collate(batch, data.q_num)\n",
    "    ):\n",
    "        P = model(X)\n",
    "        Q = q_onehot[Q]\n",
    "        Q, P, Y, S = Q[:, 1:], P[:, :-1], Y[:, 1:], S[:, 1:]\n",
    "\n",
    "        P = (Q * P).sum(2)\n",
    "\n",
    "        index = S == 1\n",
    "\n",
    "        P = torch.sigmoid(P)\n",
    "        dkt_y_hat = P[index]\n",
    "        y = Y[index]\n",
    "\n",
    "        # 在这里吧输出数据当初分类网络的输入数据，主要是把输出数据进行分类\n",
    "        class_input_size = 1  # 此时上游输出已经是各个题目的掌握程度\n",
    "        class_rnn_hidden_size = 128  # 这个\n",
    "        class_rnn = nn.RNN(input_size=class_input_size, hidden_size=class_rnn_hidden_size, num_layers=2,\n",
    "                           batch_first=True)\n",
    "        class_rnn_fc = nn.Sequential(\n",
    "            # 激活函数\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(class_rnn_hidden_size, 3),\n",
    "            nn.Softmax(dim=2),\n",
    "        )\n",
    "        length = dkt_y_hat.shape[0]\n",
    "        new_shape = (1, length, 1)\n",
    "        dkt_y_hat = torch.reshape(dkt_y_hat, new_shape)\n",
    "        size = dkt_y_hat.shape[0]\n",
    "        dkt_y_hat, hidden = class_rnn(dkt_y_hat, zeros(2, size, class_rnn_hidden_size))\n",
    "        dkt_y_hat = class_rnn_fc(dkt_y_hat)\n",
    "        dkt_y_hat = torch.reshape(dkt_y_hat, (length, 3))\n",
    "\n",
    "        y_pred += detach(dkt_y_hat)\n",
    "        y_true += detach(y)\n",
    "        loss += detach(criterion(dkt_y_hat, y))\n",
    "\n",
    "    # fpr:假阳性率;tpr:真阳性率;thres:减少了用于计算fpr和tpr的决策函数的阈值.\n",
    "    y_pred = [np.argmax(i) for i in y_pred]\n",
    "    # print(\"max y pred:\", max(y_pred))\n",
    "    fpr, tpr, thres = roc_curve(y_true, y_pred, pos_label=1)\n",
    "    mse_value = mean_squared_error(y_true, y_pred)\n",
    "    mae_value = mean_absolute_error(y_true, y_pred)\n",
    "    # bi_y_pred = [torch.argmax(i) for i in y_pred]\n",
    "    acc_value = accuracy_score(y_true, y_pred)\n",
    "    # auc, loss, mse, acc\n",
    "    return auc(fpr, tpr), loss, mse_value, mae_value, acc_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "uuid": "627bf6a7-ddd8-432f-9f41-663998661a17"
   },
   "outputs": [],
   "source": [
    "# 参数　dataset：eanalyst_math/eanalyst_math；hidden_num：128；learning_rate：0.09；length：200,epochs：200,batch_size：32,seed：0,# embed_dim,\n",
    "# 　q_num：2750,cv_num：5\n",
    "\n",
    "def experiment(\n",
    "        dataset,\n",
    "        hidden_num,\n",
    "        # concept_num,\n",
    "        learning_rate,\n",
    "        length,\n",
    "        epochs,\n",
    "        batch_size,\n",
    "        seed,\n",
    "        # embed_dim,\n",
    "        q_num,\n",
    "        cv_num\n",
    "):\n",
    "    # 设置随机数生成器的种子\n",
    "    set_seed(seed)\n",
    "    # Data实例化:test_data\n",
    "    test_data = Data(open('./data/%s/ccnu_8_math_test.csv' % dataset, 'r'), length, q_num, is_test=True)\n",
    "    path = './result_dkt_cross/%s' % ('{0:%Y-%m-%d-%H-%M-%S}'.format(datetime.datetime.now()))\n",
    "    os.makedirs(path)\n",
    "    info_file = open('%s/info.txt' % path, 'w+')\n",
    "\n",
    "    params_list = (\n",
    "        'dataset = %s\\n' % dataset,\n",
    "        'hidden_size = %d\\n' % hidden_num,\n",
    "        # 'concept_num = %d\\n' % concept_num,\n",
    "        'learning_rate = %f\\n' % learning_rate,\n",
    "        'length = %d\\n' % length,\n",
    "        'batch_size = %d\\n' % batch_size,\n",
    "        'seed = %d\\n' % seed,\n",
    "        'q_num = %d\\n' % q_num\n",
    "    )\n",
    "    info_file.write('file_name = allxt-onehot no norm + weight decay 5e-4')\n",
    "    info_file.write('%s%s%s%s%s%s%s' % params_list)\n",
    "\n",
    "    total_auc = 0.0\n",
    "    model_list = []\n",
    "\n",
    "    for cv in range(cv_num):\n",
    "        # ccnu 数据集里有522个学生\n",
    "        origin_list = [i for i in range(190)]\n",
    "        # valid set index\n",
    "        # 使随机数生成器的种子不同，从而使每次迭代分割出不同的验证集\n",
    "        random.seed(cv + 1000)\n",
    "        # random.sample(range(100), 10)：从100个数中不重复随机抽样10个数，这里是五倍交叉验证\n",
    "        index_split = random.sample(origin_list, int(0.2 * len(origin_list)))\n",
    "        random.seed(0)\n",
    "\n",
    "        train_data = Data(open('./data/%s/ccnu_8_math_train.csv' % dataset, 'r'), length, q_num, is_test=False,\n",
    "                          index_split=index_split, is_train=True)\n",
    "        valid_data = Data(open('./data/%s/ccnu_8_math_train.csv' % dataset, 'r'), length, q_num, is_test=False,\n",
    "                          index_split=index_split, is_train=False)\n",
    "        max_auc = 0.0\n",
    "        # DKT模型实例化：model\n",
    "        model = cuda(DKT(train_data.q_num, hidden_num))\n",
    "\n",
    "        # torch.optim.SGD：类，实现随机梯度下降（可选带动量）的优化器,params：用于优化的参数迭代或定义参数组的dicts，momentum:动量因子\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "        lambda1 = lambda epoch: epoch // 30\n",
    "        lambda2 = lambda epoch: 0.95 ** epoch\n",
    "        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda2)\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            time_start = time.time()\n",
    "            train(model, train_data, optimizer, batch_size)\n",
    "            train_auc, train_loss, train_mse, train_mae, train_acc = evaluate(model, train_data, batch_size)\n",
    "            valid_auc, valid_loss, valid_mse, valid_mae, valid_acc = evaluate(model, valid_data, batch_size)\n",
    "            time_end = time.time()\n",
    "\n",
    "            if max_auc < valid_auc:\n",
    "                max_auc = valid_auc\n",
    "                torch.save(model.state_dict(), '%s/model_%s' % ('%s' % path, '%d' % cv))\n",
    "                current_max_model = model\n",
    "\n",
    "            print_list = (\n",
    "                'cv:%-3d' % cv,\n",
    "                'epoch:%-3d' % epoch,\n",
    "                'max_auc:%-8.4f' % max_auc,\n",
    "                'valid_auc:%-8.4f' % valid_auc,\n",
    "                'valid_loss:%-8.4f' % valid_loss,\n",
    "                'valid_mse:%-8.4f' % valid_mse,\n",
    "                'valid_mae:%-8.4f' % valid_mae,\n",
    "                'valid_acc:%-8.4f' % valid_acc,\n",
    "                'train_auc:%-8.4f' % train_auc,\n",
    "                'train_loss:%-8.4f' % train_loss,\n",
    "                'train_mse:%-8.4f' % train_mse,\n",
    "                'train_mae:%-8.4f' % train_mae,\n",
    "                'train_acc:%-8.4f' % train_acc,\n",
    "                'time:%-6.2fs' % (time_end - time_start)\n",
    "            )\n",
    "\n",
    "            print('%s %s %s %s %s %s %s %s %s %s %s %s %s %s' % print_list)\n",
    "            info_file.write('%s %s %s %s %s %s %s %s %s %s %s %s %s %s\\n' % print_list)\n",
    "        model_list.append(current_max_model)\n",
    "\n",
    "    # 模型测试\n",
    "    train_list = []\n",
    "    auc_list = []\n",
    "    mse_list = []\n",
    "    mae_list = []\n",
    "    acc_list = []\n",
    "    loss_list = []\n",
    "    # enumerate() 函数:用于将一个可遍历的数据对象(如列表、元组或字符串)组合为一个索引序列，同时列出索引和序列元素，一般用在 for 循环当中。\n",
    "    for cv, model_item in enumerate(model_list):\n",
    "        train_auc, train_loss, train_mse, train_mae, train_acc = evaluate(model_item, train_data, batch_size)\n",
    "        test_auc, test_loss, test_mse, test_mae, test_acc = evaluate(model_item, test_data, batch_size)\n",
    "\n",
    "        train_list.append(train_auc)\n",
    "        auc_list.append(test_auc)\n",
    "        mse_list.append(test_mse)\n",
    "        mae_list.append(test_mae)\n",
    "        acc_list.append(test_acc)\n",
    "        loss_list.append(test_loss)\n",
    "        print_list_test = (\n",
    "            'cv:%-3d' % cv,\n",
    "            'train_auc:%-8.4f' % train_auc,\n",
    "            'test_auc:%-8.4f' % test_auc,\n",
    "            'test_mse:%-8.4f' % test_mse,\n",
    "            'test_mae:%-8.4f' % test_mae,\n",
    "            'test_acc:%-8.4f' % test_acc,\n",
    "            'test_loss:%-8.4f' % test_loss\n",
    "        )\n",
    "\n",
    "        print('%s %s %s %s %s %s %s\\n' % print_list_test)\n",
    "        info_file.write('%s %s %s %s %s %s %s\\n' % print_list_test)\n",
    "\n",
    "    average_train_auc = sum(train_list) / len(train_list)\n",
    "    average_test_auc = sum(auc_list) / len(auc_list)\n",
    "    average_test_mse = sum(mse_list) / len(mse_list)\n",
    "    average_test_mae = sum(mae_list) / len(mae_list)\n",
    "    average_test_acc = sum(acc_list) / len(acc_list)\n",
    "    average_test_loss = sum(loss_list) / len(loss_list)\n",
    "    print_result = (\n",
    "        'average_train_auc:%-8.4f' % average_train_auc,\n",
    "        'average_test_auc:%-8.4f' % average_test_auc,\n",
    "        'average_test_mse:%-8.4f' % average_test_mse,\n",
    "        'average_test_mae:%-8.4f' % average_test_mae,\n",
    "        'average_test_acc:%-8.4f' % average_test_acc,\n",
    "        'average_test_loss:%-8.4f' % average_test_loss\n",
    "    )\n",
    "    print('%s %s %s %s %s %s\\n' % print_result)\n",
    "    info_file.write('%s %s %s %s %s %s\\n' % print_result)\n",
    "\n",
    "\n",
    "# ### 运行程序-使用命令行参数传入相关参数\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# 数据集：assist2009, synthetic, assist2015, STATICS，assist2012，eanalyst\n",
    "# 创建解析步骤\n",
    "parser = argparse.ArgumentParser(description='Script to test DKT.')\n",
    "# 添加参数步骤\n",
    "parser.add_argument('--dataset', type=str, default='ccnu_data', help='')\n",
    "parser.add_argument('--hidden_num', type=int, default=128, help='')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.01, help='')\n",
    "parser.add_argument('--length', type=int, default=200, help='')\n",
    "parser.add_argument('--epochs', type=int, default=100, help='')\n",
    "parser.add_argument('--batch_size', type=int, default=32, help='')\n",
    "parser.add_argument('--seed', type=int, default=0, help='')\n",
    "parser.add_argument('--q_num', type=int, default=260, help='')\n",
    "parser.add_argument('--cv_num', type=int, default=5, help='')\n",
    "\n",
    "# 参数中加入args=[]\n",
    "params = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "uuid": "50b4ca61-ec70-412c-9a0b-1e178a9495a2"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-28e15b3b5d84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;31m# sklearn.metrics:包含了许多模型评估指标，例如决定系数R2、准确度等;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# roc_curve:roc曲线；mean_squared_error：均方差；mean_absolute_error：平均绝对误差；accuracy_score:准确率\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "experiment(\n",
    "    dataset=params.dataset,\n",
    "    hidden_num=params.hidden_num,\n",
    "    learning_rate=params.learning_rate,\n",
    "    length=params.length,\n",
    "    epochs=params.epochs,\n",
    "    batch_size=params.batch_size,\n",
    "    seed=params.seed,\n",
    "    q_num=params.q_num,\n",
    "    cv_num=params.cv_num\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "uuid": "216ef03b-4be5-4e1c-b478-f4ecc745878d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "uuid": "a376c854-8fbe-44f5-ab09-c9f06e927cc0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
